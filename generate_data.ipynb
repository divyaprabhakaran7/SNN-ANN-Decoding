{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef90846-a73d-4a87-ac03-0a1bff9ee556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db3f9ca-7ff2-4584-9aa1-8e12bb42927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input spike data\n",
    "mat = scipy.io.loadmat(Path('data') / '01.mat') # animal 01\n",
    "\n",
    "spike_train_all = mat['resp_train'] # spike train of all neurons, neurons x image x trials x milliseconds\n",
    "\n",
    "images_all = mat['images'].squeeze()\n",
    "images_all = torch.stack([torch.tensor(entry) for entry in images_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f3848d7-d44e-4e5f-84d9-6c12b73c82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep well-centered channels\n",
    "indcent = mat['INDCENT'].squeeze()\n",
    "spike_train_cent = torch.tensor(spike_train_all[indcent == 1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba2f8fb-e9e1-424e-8609-512432bb620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of all small natural images\n",
    "idx_small_nat_images = torch.zeros(spike_train_all.shape[1], dtype=torch.bool)\n",
    "idx_small_nat_images[:539:2] = 1\n",
    "\n",
    "# get indices of all big natural images\n",
    "idx_big_nat_images = torch.ones(spike_train_all.shape[1], dtype=torch.bool)\n",
    "idx_big_nat_images[:539:2] = 0\n",
    "idx_big_nat_images[540:] = 0\n",
    "\n",
    "# get indices of all gratings\n",
    "idx_gratings = torch.zeros(spike_train_all.shape[1], dtype=torch.bool)\n",
    "idx_gratings[540:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e725aa8-b86a-4ccb-b01e-f89340862d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use gratings\n",
    "spike_train_cent = spike_train_cent[:, idx_gratings, :, :]\n",
    "images_all = images_all[idx_gratings, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1da37ec-aaf7-4bd1-a997-a957e1b7d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = len(images_all)\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a21caa-0785-46e6-b74c-d4e53ff930fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate shuffled indices\n",
    "indices = np.arange(n_images)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e90168-9502-4f4e-81c3-191876e5e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute split sizes\n",
    "train_size = int(train_frac * n_images)\n",
    "val_size = int(val_frac * n_images)\n",
    "test_size = int(test_frac * n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f231989-08ae-4c9b-bcfb-a0fa2b506535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split indices\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc36ffe4-c752-4902-9998-0f7864feedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean masks\n",
    "train_mask = np.zeros(n_images, dtype=bool)\n",
    "val_mask = np.zeros(n_images, dtype=bool)\n",
    "test_mask = np.zeros(n_images, dtype=bool)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f6cd045-6660-468d-8860-b6f444e17747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, validation, and test sets\n",
    "train_images = images_all[train_mask, :, :]\n",
    "val_images = images_all[val_mask, :, :]\n",
    "test_images = images_all[test_mask, :, :]\n",
    "\n",
    "train_spikes = spike_train_cent[:, train_mask, :, :]\n",
    "val_spikes = spike_train_cent[:, val_mask, :, :]\n",
    "test_spikes = spike_train_cent[:, test_mask, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7638867f-8b3d-43aa-b8ab-e2b232333c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "save_dir = Path('data')\n",
    "\n",
    "torch.save(train_images, Path(save_dir) / 'train_images_gratings.pt')\n",
    "torch.save(val_images, Path(save_dir) / 'val_images_gratings.pt')\n",
    "torch.save(test_images, Path(save_dir) / 'test_images_gratings.pt')\n",
    "\n",
    "torch.save(train_spikes, Path(save_dir) / 'train_spikes_gratings.pt')\n",
    "torch.save(val_spikes, Path(save_dir) / 'val_spikes_gratings.pt')\n",
    "torch.save(test_spikes, Path(save_dir) / 'test_spikes_gratings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fc8662-142f-4cd2-9e18-4369c98539fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train spikes shape: torch.Size([26, 332, 20, 106])\n",
      "Train images shape: torch.Size([332, 320, 320])\n",
      "\n",
      "Image 0:\n",
      "Active neurons (firing rate > threshold): [1, 5, 11, 12, 17, 19, 22]\n",
      "Bar center location: (159.0, 159.0)\n",
      "\n",
      "Image 1:\n",
      "Active neurons (firing rate > threshold): [0, 1, 5, 6, 7, 12, 16]\n",
      "Bar center location: (159.0, 159.0)\n",
      "\n",
      "Image 2:\n",
      "Active neurons (firing rate > threshold): [0, 1, 5, 6, 12, 14, 16]\n",
      "Bar center location: (159.0, 159.0)\n",
      "\n",
      "Image 3:\n",
      "Active neurons (firing rate > threshold): [1, 8, 11, 12, 16, 19]\n",
      "Bar center location: (159.0, 159.0)\n",
      "\n",
      "Image 4:\n",
      "Active neurons (firing rate > threshold): [1, 2, 6, 11, 12]\n",
      "Bar center location: (159.0, 159.0)\n"
     ]
    }
   ],
   "source": [
    "train_spikes = torch.load('/Users/Divya/SNN Decoding - ANN/data/train_spikes_gratings.pt')\n",
    "train_images = torch.load('/Users/Divya/SNN Decoding - ANN/data/train_images_gratings.pt')\n",
    "\n",
    "print(\"Train spikes shape:\", train_spikes.shape)  # [26, 332, 20, 106]\n",
    "print(\"Train images shape:\", train_images.shape)  # [332, 320, 320]\n",
    "\n",
    "# For the first few images:\n",
    "for i in range(5):\n",
    "    # Get the spike patterns for this image\n",
    "    spikes = train_spikes[:, i, :, :]  # [26 neurons, 20 trials, 106 timepoints]\n",
    "    \n",
    "    # Get corresponding image\n",
    "    image = train_images[i]  # [320, 320]\n",
    "    \n",
    "    # Calculate average firing rate per neuron\n",
    "    firing_rates = spikes.mean(dim=(1,2))  # Average across trials and time\n",
    "    \n",
    "    # We could print:\n",
    "    print(f\"\\nImage {i}:\")\n",
    "    print(\"Active neurons (firing rate > threshold):\", \n",
    "          torch.where(firing_rates > firing_rates.mean())[0].tolist())\n",
    "    \n",
    "    # Find bar location in image (look for non-background pixels)\n",
    "    bar_pixels = torch.where(image != image[0,0])  # Get coordinates of the bar\n",
    "    if len(bar_pixels[0]):\n",
    "        center_y = bar_pixels[0].float().mean()\n",
    "        center_x = bar_pixels[1].float().mean()\n",
    "        print(f\"Bar center location: ({center_x:.1f}, {center_y:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc726df3-b2ae-410b-8ff4-00d2d2f6e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few images:\n",
      "\n",
      "Image 0 unique values: [59, 61, 63, 65, 67, 69, 71, 74, 76, 78, 80, 83, 85, 87, 90, 92, 95, 97, 100, 102, 105, 107, 110, 112, 115, 116, 117, 120, 122, 125, 127, 130, 132, 135, 137, 140, 142, 145, 147, 149, 152, 154, 156, 158, 161, 163, 165, 167, 169, 171]\n",
      "Number of non-background pixels: 1941\n",
      "\n",
      "Image 1 unique values: [116, 174, 176, 178, 179, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]\n",
      "Number of non-background pixels: 1941\n",
      "\n",
      "Image 2 unique values: [61, 63, 65, 67, 69, 71, 74, 76, 78, 80, 83, 85, 87, 90, 92, 95, 97, 100, 102, 105, 107, 110, 112, 115, 116, 117, 120, 122, 125, 127, 130, 132, 135, 137, 140, 142, 145, 147, 149, 152, 154, 156, 158, 161, 163, 165, 167, 169, 171, 173]\n",
      "Number of non-background pixels: 1941\n",
      "\n",
      "Spike patterns:\n",
      "\n",
      "Image 0:\n",
      "Neuron 1 peak spike times: [99, 103, 102, 101, 100]\n",
      "Neuron 5 peak spike times: [104, 101, 98, 97, 59]\n",
      "\n",
      "Image 1:\n",
      "Neuron 0 peak spike times: [101, 98, 80, 3, 4]\n",
      "Neuron 1 peak spike times: [4, 95, 93, 90, 97]\n"
     ]
    }
   ],
   "source": [
    "# For ground truth images\n",
    "print(\"First few images:\")\n",
    "for i in range(3):\n",
    "    # Get unique values to understand image structure\n",
    "    unique_vals = torch.unique(train_images[i])\n",
    "    print(f\"\\nImage {i} unique values:\", unique_vals.tolist())\n",
    "    \n",
    "    # Get number of non-background pixels to understand bar width/length\n",
    "    background = train_images[i, 0, 0]  # Assuming corner pixel is background\n",
    "    non_background = torch.sum(train_images[i] != background)\n",
    "    print(f\"Number of non-background pixels:\", non_background.item())\n",
    "\n",
    "# For spike patterns\n",
    "print(\"\\nSpike patterns:\")\n",
    "for i in range(2):  # Look at first 2 images\n",
    "    print(f\"\\nImage {i}:\")\n",
    "    # Get active neurons\n",
    "    spikes = train_spikes[:, i, :, :]  # [26 neurons, 20 trials, 106 timepoints]\n",
    "    firing_rates = spikes.mean(dim=(1,2))  # Average across trials and time\n",
    "    active_neurons = torch.where(firing_rates > firing_rates.mean())[0].tolist()\n",
    "    \n",
    "    # For each active neuron, show spike timing\n",
    "    for neuron in active_neurons[:2]:  # Show first 2 active neurons\n",
    "        # Average across trials\n",
    "        avg_spikes = spikes[neuron].float().mean(dim=0)  # Average across trials\n",
    "        # Print timestamps of highest activity\n",
    "        peak_times = torch.argsort(avg_spikes, descending=True)[:5]\n",
    "        print(f\"Neuron {neuron} peak spike times:\", peak_times.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28166b99-7a9b-453b-8a30-d02ea5bfa022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 320, 320])\n",
      "torch.Size([41, 320, 320])\n",
      "torch.Size([43, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "val_images = torch.load('/Users/Divya/SNN Decoding - ANN/data/val_images_gratings.pt')\n",
    "print(val_images.shape)\n",
    "decoded_images = torch.load('/Users/Divya/SNN Decoding - ANN/outputs/decoded_image_val.pt')\n",
    "print(decoded_images.shape)\n",
    "test_images = val_images = torch.load('/Users/Divya/SNN Decoding - ANN/data/test_images_gratings.pt')\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd2404-2494-4063-93cf-5abbb33a92d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
